######################################################################
## This is a script that fits regression through MLE with mixture
## density.
######################################################################

## Standard Normal Case
## Lets generate the data for simulation first
set.seed(587)
x <- rexp(300, rate = 0.2)
y <- 2 + 10 * x + rnorm(300, 0, 40)
plot(x, y)

## Lets just estimate beta first
beta.mle <- function(beta, y, x){
    ll <- -sum((dnorm(y - beta[1] - beta[2] * x, log = TRUE)))
    ll
}
beta.mle(c(2, 3), y = y, x = x)

beta.est <- optim(coef(lm(y ~ x)), beta.mle, y = y, x = x)

nlm(beta.mle, c(0, 0), y= y, x = x)

## Plot the solution, doesn't look too bad
plot(x, y)
abline(beta.est$par, col = "red")

## Lets bootstrap the estimate (non-parametric)

R = 1000
est <- matrix(0, nc = 2, nr = R)
for(i in 1:R){
    ind <- sample(300, 300, replace = TRUE)
    bx <- x[ind]
    by <- y[ind]
    R.est <- optim(coef(lm(y ~ x)), beta.mle, y = by, x = bx)
    est[i,] <- R.est$par
}

par(mfrow = c(2, 1))
plot(x, y)
for(i in 1:R){
    abline(est[i, ], col = rgb(1, 0, 0, alpha = 0.3))
}
abline(a = max(est[, 1]), b = max(est[, 2]), col = "blue")
abline(a = min(est[, 1]), b = min(est[, 2]), col = "blue")

plot(est)


## Rewrite the likelihood function
beta.mle <- function(beta, y, x){
    ll <- -sum(log(dnorm(y - beta[1] - beta[2] * x, beta[3], beta[4])))
    ll
}

R = 300
est <- matrix(0, nc = 4, nr = R)
for(i in 1:R){
    ind <- sample(300, 300, replace = TRUE)
    bx <- x[ind]
    by <- y[ind]
    R.est <- optim(c(0, 0, 0, 20), beta.mle, y = by, x = bx)
    est[i,] <- R.est$par
}

plot(x,y)
for(i in 1:R){
    abline(est[i, 1:2], col = rgb(1, 0, 0, alpha = 0.3))
}
abline(a = max(est[, 1]), b = max(est[, 2]), col = "blue")
abline(a = min(est[, 1]), b = min(est[, 2]), col = "blue")
lines(lowess(y ~ x), lwd = 2, lty = 2)
abline(2, 10, lwd = 2, col = "grey")

## TODO: The Script above doesn't seem to work particularly well, need
## to find out what is going on but for now we will move on to the
## mixture case. This is fixed now, the script didn't work
## particularly well is due to the fact that the prior were set too
## small and that the likelihood function can not be evaluated.


## Simple scale mixture case
## Lets generate the data for simulation
library(lsei)
source("cnm.R")

## Generate the data points
set.seed(587)
x1 <- runif(300, 0, 10)
x2 <- runif(100, 10, 20)
x3 <- runif(50, 20, 30)
x <- c(x1, x2, x3)
y1 <- 2 + 3 * x1 + rnorm(300, 0, 3)
y2 <- 2 + 3 * x2 + rnorm(100, 0, 5)
y3 <- 2 + 3 * x3 + rnorm(50, 0, 10)
y <- c(y1, y2, y3)
plot(x, y)

## This is the residual density, we can see this is clearly
## non-gaussian, the 4.222 is the weighted average of the variance
## used in the generation
res <- (y - 2 - 3 * x)
hist(res, breaks = 50, main = "True residual distribution", freq = FALSE)
curve(dnorm(x, mean(res), 4.222), add = TRUE, col = "red")

## Lets estimate the parameters assuming that the residuals is N(0,
## sigma)
beta.mle <- function(beta, y, x){
    ll <- -sum(log(dnorm(y - beta[1] - beta[2] * x, 0, beta[3])))
    ll
}
beta.mle(c(0, 0, 20), y = y, x = x)
## TODO: need to check why the warning messgae exists.

beta.est <- optim(c(0, 0, 20), beta.mle, y = y, x = x)

## Plot the solution, doesn't line of fit is not too bad as suggested
## by the theorm, however, the residual density is clearly not
## gaussian.
par(mfrow = c(2, 1))
plot(x, y)
abline(beta.est$par[c(1, 2)], col = "red")
abline(a = beta.est$par[1] + 2 * beta.est$par[3],
       b = beta.est$par[2], col = "blue")
abline(a = beta.est$par[1] - 2 * beta.est$par[3],
       b = beta.est$par[2], col = "blue")

hist(y - beta.est$par[1] - beta.est$par[2] * x, breaks = 50,
     freq = FALSE)
curve(dnorm(x, 0, beta.est$par[3]), add = TRUE, col = "red")

## Lets bootstrap the estimate (non-parametric)
R = 1000
mix.est <- matrix(0, nc = 3, nr = R)
for(i in 1:R){
    ind <- sample(450, 450, replace = TRUE)
    bx <- x[ind]
    by <- y[ind]
    R.est <- optim(c(0, 0, 20), beta.mle, y = by, x = bx)
    mix.est[i,] <-R.est$par
}

plot(x, y, pch = 19, cex = 0.5)
for(i in 1:R){
    abline(mix.est[i, 1:2], col = rgb(1, 0, 0, alpha = 0.1))
}
abline(a = max(mix.est[, 1]), b = max(mix.est[, 2]), col = "blue")
abline(a = min(mix.est[, 1]), b = min(mix.est[, 2]), col = "blue")

## Trial for the cnm functions
mymix <- normmix(mu = c(0, 0, 0), pi = rep(1/3, 3), va = c(3, 5, 10))

## The log-likehood of the mixture distribution
regmix.ll <- function(beta, x, y, mix){
    ll <- -sum(dnormmix(mix, y - beta[1] - beta[2] * x, log = TRUE))
    ll
}


## Lets bootstrap the estimate (non-parametric)
R = 100
mix2.est <- matrix(0, nc = 2, nr = R)
for(i in 1:R){
    ind <- sample(450, 450, replace = TRUE)
    bx <- x[ind]
    by <- y[ind]
    R.est <- optim(c(0, 0), regmix.ll, y = by, x = bx, mix = mymix)
    mix2.est[i,] <-R.est$par
}

plot(x, y, pch = 19, cex = 0.5)
for(i in 1:R){
    abline(mix2.est[i, 1:2], col = rgb(1, 0, 0, alpha = 0.1))
}
abline(a = max(mix2.est[, 1]), b = max(mix2.est[, 2]), col = "blue")
abline(a = min(mix2.est[, 1]), b = min(mix2.est[, 2]), col = "blue")

## This is the fit and also the residual distribution with the fitted
## distributions, however, the strange thing is that the mixture
## distribution does not seem to fit particularly well.
hist(y - mean(mix2.est[, 1]) - mean(mix2.est[, 2]) * x, breaks = 50,
     freq = FALSE)
curve(dnormmix(mymix, x), add = TRUE, col = "red")
curve(dnorm(x, 0, beta.est$par[3]), add = TRUE, col = "blue")

######################################################################
## Estimate the semi-parametric regression using cnm
######################################################################

## obtain the support points
cnm.sp <- function(beta, x, y){
    sp <- cnm.normmix(y - beta[1] - beta[2] * x)
    sp$mix
}

## function function or estimation equation to be optimised
cnm.ll <- function(beta, x, y, mix){
    ll <- -sum(dnormmix(mix, y - beta[1] - beta[2] * x, log = TRUE))
    ll
}

## Start the estimation. However, this is CNM-AP. The parameters are
## estimated seperately
n <- 50
para <- matrix(rep(0, 2 * (n + 1)), nc = 2)
para[1, ] <- c(1, 2)
for(i in 1:n){
    mySp <- cnm.sp(para[i, ], x = x, y = y)
    para[i + 1, ] <- optim(para[i, ], cnm.ll,
                           x = x, y = y, mix = mySp)$par
}


## This is the fit and also the residual distribution with the fitted
## distributions, however, the strange thing is that the mixture
## distribution does not seem to fit particularly well.
hist(y - mean(mix2.est[, 1]) - mean(mix2.est[, 2]) * x, breaks = 100,
     freq = FALSE)
curve(dnormmix(mySp, x), add = TRUE, col = "red")
curve(dnorm(x, 0, beta.est$par[3]), add = TRUE, col = "blue")

