\documentclass{beamer}
\usepackage{amsfonts, amsmath, graphicx, verbatim}
\usetheme{Warsaw}

\title{\bf Scale Normal Mixture GARCH: \\ An Acccount for Black Swans}
\author{\em Cheng-Jan Kao\\ Supervisor: Dr Yong Wang}
\institute{Dept. of Statistics, University of Auckland}
\date{}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\frame{
  \frametitle{Outline}
  \tableofcontents
}

\section{Introduction}
\subsection{History}
\frame{
   \frametitle{History}
   \begin{itemize}
   \item Since Bob Engle's Nobel Prize winning ARCH model, the
     vast amount of researches has been focused on how to improve the
     model.
   \item A paper by Tim Bollerslev documented a brief list of 110
     generalisation or extension to the orginal ARCH/GARCH type model.
   \item In this paper we build on existing work on heavy tail GARCH
     model by relaxing the conditional error distribution from a
     Normal to a Scale normal mixture.
   \end{itemize}
}
\subsection{Motivation}
\frame{
   \frametitle{Motivation}
   \begin{itemize}
   \item There has been much criticism on the GARCH frame work despite
     its popularity. One of them is the inability to capture the heavy
     tail nature of the financial time series.
   \item The aim of the work attmpts to address this shortfall by
     extending a single normal error distribution to a mixture of
     normal distribution with different variance.
   \item The generalisation not only improves the fit and the account
     for heavy tail, it also enable the understanding of the market
     structure.
   \end{itemize}
}

\section{Background}
\subsection{GARCH}
\frame{
   \frametitle{The genral GARCH(p, q) model}
   \begin{block}{GARCH(p, q)}
   \begin{align*}
     x_t &= \mu_t + \sigma_t \epsilon_t \;\;\;\;\;\;\;\;\;\;\;
            \epsilon_t \sim N(0, 1)\\
     \sigma_t^2 &= \alpha_0 + \sum_{i = 1}^p \beta_i \sigma_{t-i}^2 +
      \sum_{j = 1}^q \alpha_j x_{t-j}^2\\
   \end{align*}
   \end{block}
}

\subsection{Scale Normal Mixture}
\frame{
\frametitle{Mixture Distribution}
    In it's most simplistic form, the Scaled Normal Mixture(SNM) is
    just a mixture of Normal distribuions with different scale
    parameter ($\theta$).
    \begin{block}{Scale Normal Mixture}
    $$
    f(y_i|x_i; \pi, \theta, \beta) = \sum_{i = 1}^m \pi_j f(y_i|x_i;
    \theta_j, \beta)
    $$
    \end{block}
}

\section{Scale Normal Mixture GARCH}
\subsection{Model}
\frame{
   \frametitle{Scale Normal Mixture GARCH (p, q)}
   A scaled normal mixture GARCH model is simply a GARCH model where
   the assumption of standard normal error distribution is replaced
   with a scale normal mixture.
    \begin{block}{Scale Normal Mixture GARCH}
    \begin{align*}
     x_t &= \mu_t + \sigma_t \epsilon_t \;\;\;\;\;\;\;\;\;\;\;
            \epsilon_t \sim SNM(0, G)\\
     \sigma_t^2 &= \alpha_0 + \sum_{i = 1}^p \beta_i \sigma_{t-i}^2 +
      \sum_{j = 1}^q \alpha_j x_{t-j}^2\\
     \end{align*}
     \end{block}
}

\subsection{Estimation}
\frame{
    \frametitle{Estimation}
    The estimation is done via the lsei package by Yong (2010)
    which estimates the semi-parametric model using maximum likelihood
    estimation. The following functions forms a gradient surface
    which the algorithm maximise to obtain the estimated parameters.
}

\frame{
    \frametitle{Objective functions}
}

\section{Examples}
\frame{
   \frametitle{The NYSE}
}


\frame{
    \frametitle{The data}
    %\includegraphics{}
}

\frame{
    \frametitle{Fit}
}

\frame{
    \frametitle{Compasiron}
}

\section{Conclusion}
\frame{
    \frametitle{Conclustion}
}

\frame{
   \frametitle{Further Development}
}

\end{document}
